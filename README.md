## Evaluating the Adversarial Robustness of Adaptive Test-time Defenses

Francesco Croce*, Sven Gowal*, Thomas Brunner*, Evan Shelhamer*, Matthias Hein, Taylan Cemgil \
https://arxiv.org/abs/2202.13711

### Case study

We evaluate the following defenses:

+ `yoon_2021`: [Adversarial Purification with Score-based Generative Models](https://arxiv.org/abs/2106.06041)
+ `hwang_2021`: [AID-purifier: A light auxiliary network for boosting adversarial defense](https://arxiv.org/abs/2107.06456)
+ `wu_2021`: [Attacking Adversarial Attacks as A Defense](https://arxiv.org/abs/2106.04938)
+ `shi_2020`: [Online Adversarial Purification based on Self-Supervision](https://arxiv.org/abs/2101.09387)
+ `kang_2021`: [Stable Neural ODE with Lyapunov-Stable Equilibrium Points for Defending against Adversarial Attacks](https://arxiv.org/abs/2110.12976)
+ `mao_2021`: [Adversarial Attacks are Reversible with Natural Supervision](https://arxiv.org/abs/2103.14222)
+ `qian_2021`: [Improving Model Robustness with Latent Distribution Locally and Globally](https://arxiv.org/abs/2107.04401)
+ `alfarra_2021`: [Combating Adversaries with Anti-Adversaries](https://arxiv.org/abs/2103.14347)
+ `chen_2021`: [Towards Robust Neural Networks via Close-loop Control](https://arxiv.org/abs/2102.01862)

Some folders have a single Python notebook while other contain more involved code.
As a result, such folders will contain a `run_eval.sh` with the commands to run the evaluations or an explanatory `README.md` file.
The pre-trained models have to be downloaded following the indications in the corresponding folders and papers, 
together with the details provided in the appendix of our paper.
